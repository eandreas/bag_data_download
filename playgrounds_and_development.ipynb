{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content of download.py - FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import filecmp\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URLs = {\n",
    "    'BAG_test_data': 'https://www.bag.admin.ch/dam/bag/de/dokumente/mt/k-und-i/aktuelle-ausbrueche-pandemien/2019-nCoV/covid-19-basisdaten-labortests.xlsx.download.xlsx/Dashboard_3_COVID19_labtests_positivity.xlsx',\n",
    "    'BAG_report_data': 'https://www.bag.admin.ch/dam/bag/de/dokumente/mt/k-und-i/aktuelle-ausbrueche-pandemien/2019-nCoV/covid-19-datengrundlage-lagebericht.xlsx.download.xlsx/200325_Datengrundlage_Grafiken_COVID-19-Bericht.xlsx',\n",
    "    'BAG_cases_data': 'https://www.bag.admin.ch/dam/bag/de/dokumente/mt/k-und-i/aktuelle-ausbrueche-pandemien/2019-nCoV/covid-19-basisdaten-fallzahlen.xlsx.download.xlsx/Dashboards_1&2_COVID19_swiss_data_pv.xlsx',\n",
    "    'BAG_covid19_website': 'https://www.covid19.admin.ch'\n",
    "    \n",
    "}\n",
    "\n",
    "def download(url, target_dir = Path.cwd(), file_name = None, overwrite = False):\n",
    "    '''\n",
    "    Downloads a file from an url into target_dir. If no file_name is probided, the file is named\n",
    "    as defined by the url. In case there is already a file named file_name within target_dir, overwrite=True\n",
    "    needs to be set to force saving the download.\n",
    "    '''\n",
    "    # get the file name from url if fn is None\n",
    "    if file_name is None:\n",
    "        file_name = url.split('/')[-1]\n",
    "    # exit if the file already exists and overwrite = False\n",
    "    f = target_dir / file_name\n",
    "    if (f.exists() and not overwrite):\n",
    "        return\n",
    "    # download and save the file\n",
    "    r = requests.get(url, allow_redirects=True)\n",
    "    open(f, 'wb').write(r.content)\n",
    "    return f\n",
    "\n",
    "    \n",
    "def download_if_new(url, target_dir, suffix = ''):\n",
    "    '''\n",
    "    Downloads a file fro url and stores it in target_dir unless there is already a file with\n",
    "    the same content (byte-by-byte comparison).\n",
    "    '''\n",
    "    # get the last modified file\n",
    "    try:\n",
    "        time, latest = max((f.stat().st_mtime, f) for f in target_dir.glob('*' + suffix))\n",
    "    except ValueError as e:\n",
    "        latest = None\n",
    "    # download the current file from bag\n",
    "    f_download = download(url, target_dir, file_name = 'tmp', overwrite = True)\n",
    "    \n",
    "    # compare the latest file with the current download\n",
    "    if (latest is None):\n",
    "        same = False\n",
    "    else:\n",
    "        same = filecmp.cmp(str(latest), str(f_download), shallow = False)\n",
    "    \n",
    "    # rename or remove the current download if defferent from the previous file\n",
    "    if not same:\n",
    "        prefix = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "        f_new = target_dir / (prefix + '_' + url.split('/')[-1])\n",
    "        f_download.replace(f_new)\n",
    "    else:\n",
    "        f_download.unlink()\n",
    "        \n",
    "def get_csv_url(website, append_to_website = False):  \n",
    "    page = requests.get(website)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    # Extract and store in top_items according to instructions on the left\n",
    "    links = soup.select('a')\n",
    "    url = None\n",
    "    for link in soup.select('a'):\n",
    "        text = link.text\n",
    "        text = text.strip() if text is not None else ''\n",
    "        if (text == 'Daten als .csv'):\n",
    "            url = link.get('href')\n",
    "            url = url.strip() if url is not None else ''\n",
    "            break\n",
    "    \n",
    "    if (append_to_website):\n",
    "        url = website + url\n",
    "\n",
    "    return url\n",
    "\n",
    "# download new data\n",
    "download_if_new(URLs['BAG_report_data'], Path('downloads/report_data'), suffix = '.xlsx')\n",
    "download_if_new(URLs['BAG_test_data'], Path('downloads/test_data'), suffix = '.xlsx')\n",
    "download_if_new(URLs['BAG_cases_data'], Path('downloads/cases_data'), suffix = '.xlsx')\n",
    "\n",
    "csv_url = get_csv_url(URLs['BAG_covid19_website'], append_to_website = True)\n",
    "download_if_new(csv_url, Path('downloads/csv_data'), suffix = '.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.covid19.admin.ch/api/data/20201217-p0ck9f89/downloads/sources-csv.zip\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "# Make a request\n",
    "page = requests.get(\n",
    "    \"https://www.covid19.admin.ch/de/overview/\")\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "bas_url = 'https://www.covid19.admin.ch'\n",
    "\n",
    "# Extract and store in top_items according to instructions on the left\n",
    "links = soup.select('a')\n",
    "for ahref in links:\n",
    "    text = ahref.text\n",
    "    text = text.strip() if text is not None else ''\n",
    "\n",
    "    href = ahref.get('href')\n",
    "    href = href.strip() if href is not None else ''\n",
    "    all_links.append({\"href\": href, \"text\": text})\n",
    "\n",
    "for link in soup.select('a'):\n",
    "    text = link.text\n",
    "    text = text.strip() if text is not None else ''\n",
    "    if (text == 'Daten als .csv'):\n",
    "        url = link.get('href')\n",
    "\n",
    "url = bas_url + url\n",
    "print(url)\n",
    "\n",
    "download_if_new(url, Path.cwd(), suffix='.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mcases_data\u001b[m\u001b[m/                        playgrounds_and_development.ipynb\r\n",
      "\u001b[34mcsv\u001b[m\u001b[m/                               \u001b[34mreport_data\u001b[m\u001b[m/\r\n",
      "download.py                        \u001b[34mtest_data\u001b[m\u001b[m/\r\n",
      "\u001b[34mdownloads\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
